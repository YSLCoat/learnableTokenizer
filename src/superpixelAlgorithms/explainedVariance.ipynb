{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import thin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "\n",
    "\n",
    "cmap = mpl.colors.ListedColormap(torch.rand(256**2, 3).numpy())\n",
    "\n",
    "def plot_segmentation_boundaries(image_np, output_mask, figsize=(15, 15)):\n",
    "    \"\"\"\n",
    "    Plot the boundaries of the segmentation over the original image.\n",
    "    Uses pixel comparison to detect boundaries and applies thinning.\n",
    "    \"\"\"\n",
    "    output_mask_np = output_mask[0].cpu().numpy()\n",
    "\n",
    "    # Find boundaries by comparing neighboring pixels\n",
    "    boundaries = np.zeros_like(output_mask_np)\n",
    "    boundaries[1:, :] = np.logical_or(boundaries[1:, :], output_mask_np[1:, :] != output_mask_np[:-1, :])  # Compare vertically\n",
    "    boundaries[:, 1:] = np.logical_or(boundaries[:, 1:], output_mask_np[:, 1:] != output_mask_np[:, :-1])  # Compare horizontally\n",
    "\n",
    "    # Apply thinning to ensure boundaries are only 1 pixel wide\n",
    "    boundaries = thin(boundaries)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    ax.imshow(image_np)\n",
    "\n",
    "    ax.contour(boundaries, colors='red', linewidths=0.7)\n",
    "\n",
    "    ax.set_title('Image with Segmentation Boundaries (Thinned)')\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_mask(mask):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask[0].cpu().numpy(), cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    plt.title('Superpixel Mask')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_gradient_map(grad_map):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grad_map[0, 0].cpu().numpy(), cmap='gray')\n",
    "    plt.colorbar()\n",
    "    plt.title('Gradient Map')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "class BSDS500Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.images_dir = os.path.join(root_dir, 'images', split)\n",
    "        self.ground_truth_dir = os.path.join(root_dir, 'ground_truth', split)\n",
    "        self.image_files = [f for f in os.listdir(self.images_dir) if f.endswith('.jpg')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        gt_name = os.path.join(self.ground_truth_dir, self.image_files[idx].replace('.jpg', '.mat'))\n",
    "        gt_data = sio.loadmat(gt_name)\n",
    "        ground_truth = gt_data['groundTruth'][0][0][0][0][1]\n",
    "\n",
    "        \n",
    "        #print(ground_truth)\n",
    "        # print(ground_truth[0, 0])\n",
    "        # print(ground_truth[0, 0]['Segmentation'])\n",
    "        segmentation = ground_truth\n",
    "        \n",
    "        if isinstance(segmentation, np.ndarray) and segmentation.shape == (1, 1):\n",
    "            segmentation = segmentation[0, 0]\n",
    "        \n",
    "        segmentation = Image.fromarray(segmentation)\n",
    "        segmentation = segmentation.resize((224, 224), Image.NEAREST)\n",
    "        \n",
    "        segmentation = np.array(segmentation, dtype=np.int64)\n",
    "\n",
    "        segmentation = torch.tensor(segmentation, dtype=torch.long)\n",
    "        \n",
    "        return image, segmentation\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
    "])\n",
    "\n",
    "dataset_train = BSDS500Dataset(root_dir=r'D:\\Data\\BSDS500\\data', split='train', transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset_train, batch_size=10, shuffle=True, num_workers=0)\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import segmentation_models_pytorch as smp\n",
    "import math\n",
    "\n",
    "\n",
    "class VoronoiPropagation(nn.Module):\n",
    "    def __init__(self, num_clusters=64, n_channels=3, height=224, width=224, device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_clusters (int): Number of clusters (centroids) to initialize.\n",
    "            height (int): Height of the input image.\n",
    "            width (int): Width of the input image.\n",
    "            device (str): Device to run the model ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        super(VoronoiPropagation, self).__init__()\n",
    "        \n",
    "        self.C = num_clusters\n",
    "        self.H = height\n",
    "        self.W = width\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        self.unet = smp.Unet(encoder_name=\"efficientnet-b0\",\n",
    "                             encoder_weights=None,  \n",
    "                             in_channels=n_channels,               \n",
    "                             classes=n_channels)   \n",
    "        \n",
    "        # Set bandwidth / sigma for kernel\n",
    "        self.std = self.C / (self.H * self.W)**0.5\n",
    "        \n",
    "        self.convert_to_greyscale = torchvision.transforms.Grayscale(num_output_channels=1)\n",
    "\n",
    "    def compute_gradient_map(self, x):\n",
    "        # Sobel kernels for single-channel input\n",
    "        sobel_x = torch.tensor([[[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]]], device=x.device, dtype=x.dtype)\n",
    "        sobel_y = torch.tensor([[[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]]], device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        # Apply Sobel filters\n",
    "        grad_x = F.conv2d(x, sobel_x, padding=1)\n",
    "        grad_y = F.conv2d(x, sobel_y, padding=1)\n",
    "        \n",
    "        # Compute gradient magnitude\n",
    "        grad_map = torch.sqrt(grad_x.pow(2) + grad_y.pow(2))\n",
    "        return grad_map\n",
    "\n",
    "    def place_centroids_on_grid(self, batch_size):\n",
    "        num_cols = int(math.sqrt(self.C * self.W / self.H))\n",
    "        num_rows = int(math.ceil(self.C / num_cols))\n",
    "\n",
    "        grid_spacing_y = self.H / num_rows\n",
    "        grid_spacing_x = self.W / num_cols\n",
    "\n",
    "        centroids = []\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_cols):\n",
    "                if len(centroids) >= self.C:\n",
    "                    break\n",
    "                y = int((i + 0.5) * grid_spacing_y)\n",
    "                x = int((j + 0.5) * grid_spacing_x)\n",
    "                centroids.append([y, x])\n",
    "            if len(centroids) >= self.C:\n",
    "                break\n",
    "\n",
    "        centroids = torch.tensor(centroids, device=self.device).float()\n",
    "        return centroids.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "    def find_nearest_minima(self, centroids, grad_map, neighborhood_size=10):\n",
    "        updated_centroids = []\n",
    "        B, _, _ = centroids.shape\n",
    "        \n",
    "        for batch_idx in range(B):\n",
    "            updated_centroids_batch = []\n",
    "            occupied_positions = set()\n",
    "            for centroid in centroids[batch_idx]:\n",
    "                y, x = centroid\n",
    "                y_min = max(0, int(y) - neighborhood_size)\n",
    "                y_max = min(self.H, int(y) + neighborhood_size)\n",
    "                x_min = max(0, int(x) - neighborhood_size)\n",
    "                x_max = min(self.W, int(x) + neighborhood_size)\n",
    "                \n",
    "                neighborhood = grad_map[batch_idx, 0, y_min:y_max, x_min:x_max]\n",
    "                min_val = torch.min(neighborhood)\n",
    "                min_coords = torch.nonzero(neighborhood == min_val, as_tuple=False)\n",
    "                \n",
    "                # Iterate over all minima to find an unoccupied one\n",
    "                found = False\n",
    "                for coord in min_coords:\n",
    "                    new_y = y_min + coord[0].item()\n",
    "                    new_x = x_min + coord[1].item()\n",
    "                    position = (new_y, new_x)\n",
    "                    if position not in occupied_positions:\n",
    "                        occupied_positions.add(position)\n",
    "                        updated_centroids_batch.append([new_y, new_x])\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    # If all minima are occupied, keep the original position\n",
    "                    updated_centroids_batch.append([y.item(), x.item()])\n",
    "            \n",
    "            updated_centroids.append(torch.tensor(updated_centroids_batch, device=self.device))\n",
    "        \n",
    "        return torch.stack(updated_centroids, dim=0)\n",
    "\n",
    "    def distance_weighted_propagation(self, centroids, grad_map, color_map, num_iters=50, gradient_weight=10.0, color_weight=10.0, edge_exponent=4.0): # gradient weight, color weight and edge exponent are all tuneable parameters \n",
    "        \"\"\"\n",
    "        Perform Voronoi-like propagation from centroids, guided by both the gradient map and color similarity.\n",
    "        \n",
    "        Args:\n",
    "            centroids (Tensor): Initial centroid positions.\n",
    "            grad_map (Tensor): Gradient magnitude map.\n",
    "            color_map (Tensor): Input image for color similarity.\n",
    "            num_iters (int): Number of iterations to perform propagation.\n",
    "            gradient_weight (float): Weight for the gradient penalty.\n",
    "            color_weight (float): Weight for the color similarity penalty.\n",
    "            edge_exponent (float): Exponent to amplify edge gradients.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Final segmentation mask.\n",
    "        \"\"\"\n",
    "        B, _, H, W = grad_map.shape\n",
    "        mask = torch.full((B, H, W), fill_value=-1, device=grad_map.device)  # Label mask\n",
    "        dist_map = torch.full((B, H, W), fill_value=float('inf'), device=grad_map.device)  # Distance map\n",
    "        \n",
    "        for batch_idx in range(B):\n",
    "            for idx, (cy, cx) in enumerate(centroids[batch_idx]):\n",
    "                mask[batch_idx, int(cy), int(cx)] = idx\n",
    "                dist_map[batch_idx, int(cy), int(cx)] = 0  # Distance from centroid is 0 initially\n",
    "        \n",
    "        # 4-connected neighbors (dy, dx)\n",
    "        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "        \n",
    "        # Amplify the impact of the gradient map by multiplying it with a weight and applying a non-linear transformation\n",
    "        weighted_grad_map = (grad_map ** edge_exponent) * gradient_weight\n",
    "\n",
    "        # Perform propagation with both gradient penalties and color similarity\n",
    "        for _ in range(num_iters):\n",
    "            for dy, dx in directions:\n",
    "                # Shift the distance map in each direction\n",
    "                shifted_dist = torch.roll(dist_map, shifts=(dy, dx), dims=(1, 2))\n",
    "                shifted_mask = torch.roll(mask, shifts=(dy, dx), dims=(1, 2))\n",
    "                \n",
    "                # Calculate color distance between current pixel and centroid it is being propagated from\n",
    "                color_diff = torch.abs(color_map - torch.roll(color_map, shifts=(dy, dx), dims=(2, 3))).sum(dim=1)  # Sum over color channels\n",
    "\n",
    "                # Add the gradient map value as a weighted penalty to the distance\n",
    "                weighted_dist = shifted_dist + weighted_grad_map[:, 0, :, :] + color_diff * color_weight\n",
    "                \n",
    "                # Update the mask and distance map where the new combined distance is smaller\n",
    "                update_mask = weighted_dist < dist_map\n",
    "                dist_map[update_mask] = weighted_dist[update_mask]\n",
    "                mask[update_mask] = shifted_mask[update_mask]\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C_in, H, W = x.shape\n",
    "        \n",
    "        if C_in == 3:\n",
    "            grayscale_image = self.convert_to_greyscale(x)\n",
    "        else:\n",
    "            grayscale_image = x\n",
    "        \n",
    "        # Compute the gradient map from grayscale image\n",
    "        grad_map = self.compute_gradient_map(grayscale_image)\n",
    "        \n",
    "        # Place centroids on a grid\n",
    "        centroids = self.place_centroids_on_grid(B)\n",
    "        \n",
    "        # Move centroids to nearest local minima\n",
    "        centroids = self.find_nearest_minima(centroids, grad_map)\n",
    "        \n",
    "        # Use the color map (the original image) to guide propagation\n",
    "        spixel_features = self.unet(x)\n",
    "        \n",
    "        # Perform distance-weighted propagation with both gradient and color guidance\n",
    "        mask = self.distance_weighted_propagation(centroids, grad_map, spixel_features)\n",
    "        \n",
    "        # return grad_map, centroids, mask, spixel_features\n",
    "        return grad_map, centroids, mask, spixel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundaryPathFinder2(nn.Module):\n",
    "    def __init__(self, num_segments_row=8, num_segments_col=8, height=224, width=224, device='cpu'):\n",
    "        super(BoundaryPathFinder2, self).__init__()\n",
    "        \n",
    "        self.num_segments_row = num_segments_row\n",
    "        self.num_segments_col = num_segments_col\n",
    "        self.H = height\n",
    "        self.W = width\n",
    "        self.device = device\n",
    "        \n",
    "        self.convert_to_grayscale = torchvision.transforms.Grayscale(num_output_channels=1)\n",
    "        \n",
    "        # Sobel kernels\n",
    "        self.sobel_x = torch.tensor([[[[-1, 0, 1], \n",
    "                                  [-2, 0, 2], \n",
    "                                  [-1, 0, 1]]]], device=device, dtype=torch.float32)\n",
    "        self.sobel_y = torch.tensor([[[[-1, -2, -1], \n",
    "                                  [0, 0, 0], \n",
    "                                  [1, 2, 1]]]], device=device, dtype=torch.float32)\n",
    "    \n",
    "    def compute_gradient_map(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        if x.shape[1] == 3:\n",
    "            x = self.convert_to_grayscale(x)\n",
    "        \n",
    "        # Apply Sobel filters\n",
    "        grad_x = F.conv2d(x, self.sobel_x, padding=1)\n",
    "        grad_y = F.conv2d(x, self.sobel_y, padding=1)\n",
    "\n",
    "        # Compute gradient magnitude\n",
    "        grad_map = torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-8)\n",
    "        return grad_map  # Shape: (B, 1, H, W)\n",
    "    \n",
    "    def initialize_grid(self, batch_size):\n",
    "        # Create grid labels\n",
    "        rows = torch.arange(self.H, device=self.device).unsqueeze(1)\n",
    "        cols = torch.arange(self.W, device=self.device).unsqueeze(0)\n",
    "\n",
    "        row_labels = rows // (self.H // self.num_segments_row)\n",
    "        col_labels = cols // (self.W // self.num_segments_col)\n",
    "\n",
    "        labels = (row_labels * self.num_segments_col + col_labels).to(torch.int32)\n",
    "        labels = labels.expand(batch_size, -1, -1)  # Shape: (B, H, W)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def adjust_boundaries(self, grad_map, segmentation_mask, band_width=5):\n",
    "        \"\"\"\n",
    "        Adjust boundary lines to align with the highest gradients while keeping the number of segments constant.\n",
    "        \"\"\"\n",
    "        B, H, W = segmentation_mask.shape\n",
    "        device = grad_map.device\n",
    "\n",
    "        # Prepare indices\n",
    "        y_indices = torch.arange(H, device=device)\n",
    "        x_indices = torch.arange(W, device=device)\n",
    "\n",
    "        # Initialize boundary masks\n",
    "        boundary_masks = torch.zeros((B, H, W), dtype=torch.bool, device=device)\n",
    "\n",
    "        # Process each image in the batch\n",
    "        for b in range(B):\n",
    "            grad_map_b = grad_map[b, 0]  # Shape: (H, W)\n",
    "\n",
    "            # Vertical boundaries\n",
    "            vertical_paths = []\n",
    "            for i in range(1, self.num_segments_col):\n",
    "                x_init = i * (W // self.num_segments_col)\n",
    "                x_init = min(x_init, W - 1)\n",
    "                path = self.find_optimal_vertical_path(grad_map_b, x_init, band_width)\n",
    "                vertical_paths.append(path)\n",
    "\n",
    "            # Mark vertical boundaries\n",
    "            for path in vertical_paths:\n",
    "                boundary_masks[b, y_indices, path] = True\n",
    "\n",
    "            # Horizontal boundaries\n",
    "            horizontal_paths = []\n",
    "            for i in range(1, self.num_segments_row):\n",
    "                y_init = i * (H // self.num_segments_row)\n",
    "                y_init = min(y_init, H - 1)\n",
    "                path = self.find_optimal_horizontal_path(grad_map_b, y_init, band_width)\n",
    "                horizontal_paths.append(path)\n",
    "\n",
    "            # Mark horizontal boundaries\n",
    "            for path in horizontal_paths:\n",
    "                boundary_masks[b, path, x_indices] = True\n",
    "\n",
    "        # Use connected components labeling and reassign labels based on majority voting\n",
    "        from skimage.measure import label as skimage_label\n",
    "\n",
    "        new_segmentation_masks = []\n",
    "        for b in range(B):\n",
    "            boundary_mask_np = boundary_masks[b].cpu().numpy()\n",
    "            regions = ~boundary_mask_np\n",
    "            labeled_array = skimage_label(regions, connectivity=1)\n",
    "            initial_labels = segmentation_mask[b].cpu().numpy()\n",
    "            new_segmentation_mask = np.zeros_like(labeled_array, dtype=initial_labels.dtype)\n",
    "            for region_label in np.unique(labeled_array):\n",
    "                if region_label == 0:\n",
    "                    continue  # Skip background if any\n",
    "                region_mask = (labeled_array == region_label)\n",
    "                labels_in_region = initial_labels[region_mask]\n",
    "                if labels_in_region.size == 0:\n",
    "                    continue\n",
    "                # Majority voting to find the most common label\n",
    "                majority_label = np.bincount(labels_in_region).argmax()\n",
    "                new_segmentation_mask[region_mask] = majority_label\n",
    "            new_segmentation_masks.append(torch.from_numpy(new_segmentation_mask).to(device))\n",
    "\n",
    "        new_segmentation_masks = torch.stack(new_segmentation_masks, dim=0).to(torch.int32)\n",
    "\n",
    "        return new_segmentation_masks  # Shape: (B, H, W)\n",
    "    \n",
    "    def find_optimal_vertical_path(self, grad_map, x_init, band_width):\n",
    "        \"\"\"\n",
    "        Find the optimal vertical path around the initial x position using dynamic programming.\n",
    "        \"\"\"\n",
    "        H, W = grad_map.shape\n",
    "        device = grad_map.device\n",
    "\n",
    "        # Define band around x_init\n",
    "        x_indices = x_init + torch.arange(-band_width, band_width + 1, device=device)\n",
    "        x_indices = x_indices.clamp(0, W - 1).long()\n",
    "        num_positions = x_indices.size(0)\n",
    "\n",
    "        # Initialize cost and path matrices\n",
    "        cost = torch.full((H, num_positions), float('inf'), device=device)\n",
    "        path = torch.zeros((H, num_positions), dtype=torch.long, device=device)\n",
    "\n",
    "        # First row\n",
    "        cost[0] = -grad_map[0, x_indices]\n",
    "\n",
    "        # Precompute position indices\n",
    "        positions = torch.arange(num_positions, device=device)\n",
    "\n",
    "        # Dynamic programming\n",
    "        for y in range(1, H):\n",
    "            # Pad the previous cost for easy indexing\n",
    "            padded_prev_cost = torch.cat([\n",
    "                torch.full((1,), float('inf'), device=device),\n",
    "                cost[y - 1],\n",
    "                torch.full((1,), float('inf'), device=device)\n",
    "            ])\n",
    "\n",
    "            # Indices for possible moves: left (-1), stay (0), right (+1)\n",
    "            move_offsets = torch.tensor([-1, 0, 1], device=device)\n",
    "            neighbor_indices = positions.unsqueeze(1) + move_offsets  # Shape: (num_positions, 3)\n",
    "            neighbor_indices = neighbor_indices.clamp(0, num_positions - 1)\n",
    "\n",
    "            # Gather costs for possible moves\n",
    "            prev_costs = padded_prev_cost[neighbor_indices + 1]  # Adjust for padding\n",
    "            min_prev_costs, min_indices = prev_costs.min(dim=1)\n",
    "\n",
    "            # Update cost and path\n",
    "            cost[y] = min_prev_costs - grad_map[y, x_indices]\n",
    "            path[y] = neighbor_indices[positions, min_indices]\n",
    "\n",
    "        # Backtracking to find the optimal path\n",
    "        idx = cost[-1].argmin().item()\n",
    "        optimal_path = []\n",
    "        for y in reversed(range(H)):\n",
    "            optimal_path.append(x_indices[idx])\n",
    "            idx = path[y, idx].item()\n",
    "        optimal_path.reverse()\n",
    "        optimal_path = torch.stack(optimal_path)\n",
    "        return optimal_path  # Shape: (H,)\n",
    "        \n",
    "    def find_optimal_horizontal_path(self, grad_map, y_init, band_width):\n",
    "        \"\"\"\n",
    "        Find the optimal horizontal path around the initial y position using dynamic programming.\n",
    "        \"\"\"\n",
    "        H, W = grad_map.shape\n",
    "        device = grad_map.device\n",
    "\n",
    "        # Define band around y_init\n",
    "        y_indices = y_init + torch.arange(-band_width, band_width + 1, device=device)\n",
    "        y_indices = y_indices.clamp(0, H - 1).long()\n",
    "        num_positions = y_indices.size(0)\n",
    "\n",
    "        # Initialize cost and path matrices\n",
    "        cost = torch.full((W, num_positions), float('inf'), device=device)\n",
    "        path = torch.zeros((W, num_positions), dtype=torch.long, device=device)\n",
    "\n",
    "        # First column\n",
    "        cost[0] = -grad_map[y_indices, 0]\n",
    "\n",
    "        # Precompute position indices\n",
    "        positions = torch.arange(num_positions, device=device)\n",
    "        \n",
    "        # Dynamic programming\n",
    "        for x in range(1, W):\n",
    "            # Pad the previous cost for easy indexing\n",
    "            padded_prev_cost = torch.cat([\n",
    "                torch.full((1,), float('inf'), device=device),\n",
    "                cost[x - 1],\n",
    "                torch.full((1,), float('inf'), device=device)\n",
    "            ])\n",
    "\n",
    "            # Indices for possible moves: up (-1), stay (0), down (+1)\n",
    "            move_offsets = torch.tensor([-1, 0, 1], device=device)\n",
    "            neighbor_indices = positions.unsqueeze(1) + move_offsets  # Shape: (num_positions, 3)\n",
    "            neighbor_indices = neighbor_indices.clamp(0, num_positions - 1)\n",
    "\n",
    "            # Gather costs for possible moves\n",
    "            prev_costs = padded_prev_cost[neighbor_indices + 1]  # Adjust for padding\n",
    "            min_prev_costs, min_indices = prev_costs.min(dim=1)\n",
    "\n",
    "            # Update cost and path\n",
    "            cost[x] = min_prev_costs - grad_map[y_indices, x]\n",
    "            path[x] = neighbor_indices[positions, min_indices]\n",
    "\n",
    "        # Backtracking to find the optimal path\n",
    "        idx = cost[-1].argmin().item()\n",
    "        optimal_path = []\n",
    "        for x in reversed(range(W)):\n",
    "            optimal_path.append(y_indices[idx])\n",
    "            idx = path[x, idx].item()\n",
    "        optimal_path.reverse()\n",
    "        optimal_path = torch.stack(optimal_path)\n",
    "        return optimal_path  # Shape: (W,)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        if H != self.H or W != self.W:\n",
    "            raise ValueError(f\"Input image size must match initialized size: ({self.H}, {self.W})\")\n",
    "\n",
    "        # Compute gradient map\n",
    "        grad_map = self.compute_gradient_map(x)  # Shape: (B, 1, H, W)\n",
    "\n",
    "        # Initialize grid segmentation\n",
    "        segmentation_mask = self.initialize_grid(B)  # Shape: (B, H, W)\n",
    "\n",
    "        # Adjust boundaries\n",
    "        new_segmentation_mask = self.adjust_boundaries(grad_map, segmentation_mask)\n",
    "\n",
    "        return grad_map, segmentation_mask, new_segmentation_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "voronoi_model = VoronoiPropagation(num_clusters=256)\n",
    "pathFinder_model = BoundaryPathFinder2(num_segments_col=16,num_segments_row=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explained_variance_batch(image_batch, superpixel_labels_batch):\n",
    "    batch_size, num_channels, height, width = image_batch.shape\n",
    "    explained_variance_scores = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image = image_batch[i]  # Shape: (C, H, W)\n",
    "        superpixel_labels = superpixel_labels_batch[i]  # Shape: (H, W)\n",
    "\n",
    "        # Ensure superpixel_labels is in shape (H, W)\n",
    "        superpixel_labels = superpixel_labels.squeeze().to(image.device)\n",
    "\n",
    "        # Flatten image and labels for computation\n",
    "        image_flat = image.view(num_channels, height * width)\n",
    "        labels_flat = superpixel_labels.view(height * width)\n",
    "\n",
    "        # Compute total variance of the image across all channels\n",
    "        total_variance = image_flat.var(dim=1, unbiased=False).mean().item()\n",
    "\n",
    "        # Proceed to compute within-superpixel variance (homogeneity_score)\n",
    "        unique_labels = superpixel_labels.unique()\n",
    "        num_superpixels = unique_labels.size(0)\n",
    "\n",
    "        pixel_sums = torch.zeros((num_superpixels, num_channels), device=image.device)\n",
    "        pixel_squares = torch.zeros((num_superpixels, num_channels), device=image.device)\n",
    "        pixel_counts = torch.zeros(num_superpixels, device=image.device)\n",
    "\n",
    "        for j, label in enumerate(unique_labels):\n",
    "            mask = (labels_flat == label)\n",
    "            pixel_sums[j] = image_flat[:, mask].sum(dim=1)\n",
    "            pixel_squares[j] = (image_flat[:, mask] ** 2).sum(dim=1)\n",
    "            pixel_counts[j] = mask.sum()\n",
    "\n",
    "        pixel_means = pixel_sums / pixel_counts.unsqueeze(1)\n",
    "        pixel_variances = (pixel_squares / pixel_counts.unsqueeze(1)) - (pixel_means ** 2)\n",
    "        within_variance = pixel_variances.mean().item()\n",
    "\n",
    "        # Compute explained variance\n",
    "        explained_variance = 1 - (within_variance / total_variance)\n",
    "        explained_variance_scores.append(explained_variance)\n",
    "\n",
    "    return explained_variance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voronoi took 3.409597873687744\n",
      "Pathfinder took 13.611455917358398\n",
      "Voronoi took 3.3326168060302734\n",
      "Pathfinder took 13.238955020904541\n",
      "Voronoi took 3.3081629276275635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVoronoi took\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m-\u001b[39mstart)\n\u001b[0;32m     11\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 12\u001b[0m pathfinder_grad_map, pathfinder_segmentation_mask, pathfinder_new_segmentation_mask \u001b[38;5;241m=\u001b[39m \u001b[43mpathFinder_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPathfinder took\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m-\u001b[39mstart)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mBoundaryPathFinder2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    232\u001b[0m segmentation_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_grid(B)  \u001b[38;5;66;03m# Shape: (B, H, W)\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Adjust boundaries\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m new_segmentation_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_boundaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegmentation_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad_map, segmentation_mask, new_segmentation_mask\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mBoundaryPathFinder2.adjust_boundaries\u001b[1;34m(self, grad_map, segmentation_mask, band_width)\u001b[0m\n\u001b[0;32m     80\u001b[0m     y_init \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m (H \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_segments_row)\n\u001b[0;32m     81\u001b[0m     y_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(y_init, H \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_optimal_horizontal_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_map_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mband_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     horizontal_paths\u001b[38;5;241m.\u001b[39mappend(path)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Mark horizontal boundaries\u001b[39;00m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mBoundaryPathFinder2.find_optimal_horizontal_path\u001b[1;34m(self, grad_map, y_init, band_width)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# Gather costs for possible moves\u001b[39;00m\n\u001b[0;32m    206\u001b[0m prev_costs \u001b[38;5;241m=\u001b[39m padded_prev_cost[neighbor_indices \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Adjust for padding\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m min_prev_costs, min_indices \u001b[38;5;241m=\u001b[39m \u001b[43mprev_costs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Update cost and path\u001b[39;00m\n\u001b[0;32m    210\u001b[0m cost[x] \u001b[38;5;241m=\u001b[39m min_prev_costs \u001b[38;5;241m-\u001b[39m grad_map[y_indices, x]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "explained_variance_scores_voronoi = []\n",
    "explained_variance_scores_pathfinder = []\n",
    "\n",
    "for (image, labels) in dataloader:\n",
    "    start = time.time()\n",
    "    voronor_grad_map, voronor_centroids, voronor_mask, voronor_spixel_features = voronoi_model(image)\n",
    "    end = time.time()\n",
    "    print(\"Voronoi took\", end-start)\n",
    "    start = time.time()\n",
    "    pathfinder_grad_map, pathfinder_segmentation_mask, pathfinder_new_segmentation_mask = pathFinder_model(image)\n",
    "    end = time.time()\n",
    "    print(\"Pathfinder took\", end-start)\n",
    "    \n",
    "    explained_variance_scores_voronoi.append(explained_variance_batch(image, voronor_mask))\n",
    "    explained_variance_scores_pathfinder.append(explained_variance_batch(image, pathfinder_new_segmentation_mask))\n",
    "    \n",
    "\n",
    "print(np.mean(np.array(explained_variance_scores_voronoi)))\n",
    "print(np.mean(np.array(explained_variance_scores_pathfinder)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
