{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import thin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "\n",
    "\n",
    "cmap = mpl.colors.ListedColormap(torch.rand(256**2, 3).numpy())\n",
    "\n",
    "def plot_segmentation_boundaries(image_np, output_mask, figsize=(15, 15)):\n",
    "    \"\"\"\n",
    "    Plot the boundaries of the segmentation over the original image.\n",
    "    Uses pixel comparison to detect boundaries and applies thinning.\n",
    "    \"\"\"\n",
    "    output_mask_np = output_mask[0].cpu().numpy()\n",
    "\n",
    "    # Find boundaries by comparing neighboring pixels\n",
    "    boundaries = np.zeros_like(output_mask_np)\n",
    "    boundaries[1:, :] = np.logical_or(boundaries[1:, :], output_mask_np[1:, :] != output_mask_np[:-1, :])  # Compare vertically\n",
    "    boundaries[:, 1:] = np.logical_or(boundaries[:, 1:], output_mask_np[:, 1:] != output_mask_np[:, :-1])  # Compare horizontally\n",
    "\n",
    "    # Apply thinning to ensure boundaries are only 1 pixel wide\n",
    "    boundaries = thin(boundaries)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    ax.imshow(image_np)\n",
    "\n",
    "    ax.contour(boundaries, colors='red', linewidths=0.7)\n",
    "\n",
    "    ax.set_title('Image with Segmentation Boundaries (Thinned)')\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_mask(mask):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask[0].cpu().numpy(), cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    plt.title('Superpixel Mask')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_gradient_map(grad_map):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grad_map[0, 0].cpu().numpy(), cmap='gray')\n",
    "    plt.colorbar()\n",
    "    plt.title('Gradient Map')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "class BSDS500Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.images_dir = os.path.join(root_dir, 'images', split)\n",
    "        self.ground_truth_dir = os.path.join(root_dir, 'ground_truth', split)\n",
    "        self.image_files = [f for f in os.listdir(self.images_dir) if f.endswith('.jpg')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        gt_name = os.path.join(self.ground_truth_dir, self.image_files[idx].replace('.jpg', '.mat'))\n",
    "        gt_data = sio.loadmat(gt_name)\n",
    "        ground_truth = gt_data['groundTruth'][0][0][0][0][1]\n",
    "\n",
    "        \n",
    "        #print(ground_truth)\n",
    "        # print(ground_truth[0, 0])\n",
    "        # print(ground_truth[0, 0]['Segmentation'])\n",
    "        segmentation = ground_truth\n",
    "        \n",
    "        if isinstance(segmentation, np.ndarray) and segmentation.shape == (1, 1):\n",
    "            segmentation = segmentation[0, 0]\n",
    "        \n",
    "        segmentation = Image.fromarray(segmentation)\n",
    "        segmentation = segmentation.resize((224, 224), Image.NEAREST)\n",
    "        \n",
    "        segmentation = np.array(segmentation, dtype=np.int64)\n",
    "\n",
    "        segmentation = torch.tensor(segmentation, dtype=torch.long)\n",
    "        \n",
    "        return image, segmentation\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
    "])\n",
    "\n",
    "dataset_train = BSDS500Dataset(root_dir=r'D:\\Data\\BSDS500\\data', split='train', transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset_train, batch_size=10, shuffle=True, num_workers=0)\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import segmentation_models_pytorch as smp\n",
    "import math\n",
    "\n",
    "\n",
    "class VoronoiPropagation(nn.Module):\n",
    "    def __init__(self, num_clusters=64, n_channels=3, height=224, width=224, device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_clusters (int): Number of clusters (centroids) to initialize.\n",
    "            height (int): Height of the input image.\n",
    "            width (int): Width of the input image.\n",
    "            device (str): Device to run the model ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        super(VoronoiPropagation, self).__init__()\n",
    "        \n",
    "        self.C = num_clusters\n",
    "        self.H = height\n",
    "        self.W = width\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        self.unet = smp.Unet(encoder_name=\"efficientnet-b0\",\n",
    "                             encoder_weights=None,  \n",
    "                             in_channels=n_channels,               \n",
    "                             classes=n_channels)   \n",
    "        \n",
    "        # Set bandwidth / sigma for kernel\n",
    "        self.std = self.C / (self.H * self.W)**0.5\n",
    "        \n",
    "        self.convert_to_greyscale = torchvision.transforms.Grayscale(num_output_channels=1)\n",
    "\n",
    "    def compute_gradient_map(self, x):\n",
    "        # Sobel kernels for single-channel input\n",
    "        sobel_x = torch.tensor([[[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]]], device=x.device, dtype=x.dtype)\n",
    "        sobel_y = torch.tensor([[[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]]], device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        # Apply Sobel filters\n",
    "        grad_x = F.conv2d(x, sobel_x, padding=1)\n",
    "        grad_y = F.conv2d(x, sobel_y, padding=1)\n",
    "        \n",
    "        # Compute gradient magnitude\n",
    "        grad_map = torch.sqrt(grad_x.pow(2) + grad_y.pow(2))\n",
    "        return grad_map\n",
    "\n",
    "    def place_centroids_on_grid(self, batch_size):\n",
    "        num_cols = int(math.sqrt(self.C * self.W / self.H))\n",
    "        num_rows = int(math.ceil(self.C / num_cols))\n",
    "\n",
    "        grid_spacing_y = self.H / num_rows\n",
    "        grid_spacing_x = self.W / num_cols\n",
    "\n",
    "        centroids = []\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_cols):\n",
    "                if len(centroids) >= self.C:\n",
    "                    break\n",
    "                y = int((i + 0.5) * grid_spacing_y)\n",
    "                x = int((j + 0.5) * grid_spacing_x)\n",
    "                centroids.append([y, x])\n",
    "            if len(centroids) >= self.C:\n",
    "                break\n",
    "\n",
    "        centroids = torch.tensor(centroids, device=self.device).float()\n",
    "        return centroids.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "    def find_nearest_minima(self, centroids, grad_map, neighborhood_size=10):\n",
    "        updated_centroids = []\n",
    "        B, _, _ = centroids.shape\n",
    "        \n",
    "        for batch_idx in range(B):\n",
    "            updated_centroids_batch = []\n",
    "            occupied_positions = set()\n",
    "            for centroid in centroids[batch_idx]:\n",
    "                y, x = centroid\n",
    "                y_min = max(0, int(y) - neighborhood_size)\n",
    "                y_max = min(self.H, int(y) + neighborhood_size)\n",
    "                x_min = max(0, int(x) - neighborhood_size)\n",
    "                x_max = min(self.W, int(x) + neighborhood_size)\n",
    "                \n",
    "                neighborhood = grad_map[batch_idx, 0, y_min:y_max, x_min:x_max]\n",
    "                min_val = torch.min(neighborhood)\n",
    "                min_coords = torch.nonzero(neighborhood == min_val, as_tuple=False)\n",
    "                \n",
    "                # Iterate over all minima to find an unoccupied one\n",
    "                found = False\n",
    "                for coord in min_coords:\n",
    "                    new_y = y_min + coord[0].item()\n",
    "                    new_x = x_min + coord[1].item()\n",
    "                    position = (new_y, new_x)\n",
    "                    if position not in occupied_positions:\n",
    "                        occupied_positions.add(position)\n",
    "                        updated_centroids_batch.append([new_y, new_x])\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    # If all minima are occupied, keep the original position\n",
    "                    updated_centroids_batch.append([y.item(), x.item()])\n",
    "            \n",
    "            updated_centroids.append(torch.tensor(updated_centroids_batch, device=self.device))\n",
    "        \n",
    "        return torch.stack(updated_centroids, dim=0)\n",
    "\n",
    "    def distance_weighted_propagation(self, centroids, grad_map, color_map, num_iters=50, gradient_weight=10.0, color_weight=10.0, edge_exponent=4.0): # gradient weight, color weight and edge exponent are all tuneable parameters \n",
    "        \"\"\"\n",
    "        Perform Voronoi-like propagation from centroids, guided by both the gradient map and color similarity.\n",
    "        \n",
    "        Args:\n",
    "            centroids (Tensor): Initial centroid positions.\n",
    "            grad_map (Tensor): Gradient magnitude map.\n",
    "            color_map (Tensor): Input image for color similarity.\n",
    "            num_iters (int): Number of iterations to perform propagation.\n",
    "            gradient_weight (float): Weight for the gradient penalty.\n",
    "            color_weight (float): Weight for the color similarity penalty.\n",
    "            edge_exponent (float): Exponent to amplify edge gradients.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Final segmentation mask.\n",
    "        \"\"\"\n",
    "        B, _, H, W = grad_map.shape\n",
    "        mask = torch.full((B, H, W), fill_value=-1, device=grad_map.device)  # Label mask\n",
    "        dist_map = torch.full((B, H, W), fill_value=float('inf'), device=grad_map.device)  # Distance map\n",
    "        \n",
    "        for batch_idx in range(B):\n",
    "            for idx, (cy, cx) in enumerate(centroids[batch_idx]):\n",
    "                mask[batch_idx, int(cy), int(cx)] = idx\n",
    "                dist_map[batch_idx, int(cy), int(cx)] = 0  # Distance from centroid is 0 initially\n",
    "        \n",
    "        # 4-connected neighbors (dy, dx)\n",
    "        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "        \n",
    "        # Amplify the impact of the gradient map by multiplying it with a weight and applying a non-linear transformation\n",
    "        weighted_grad_map = (grad_map ** edge_exponent) * gradient_weight\n",
    "\n",
    "        # Perform propagation with both gradient penalties and color similarity\n",
    "        for _ in range(num_iters):\n",
    "            for dy, dx in directions:\n",
    "                # Shift the distance map in each direction\n",
    "                shifted_dist = torch.roll(dist_map, shifts=(dy, dx), dims=(1, 2))\n",
    "                shifted_mask = torch.roll(mask, shifts=(dy, dx), dims=(1, 2))\n",
    "                \n",
    "                # Calculate color distance between current pixel and centroid it is being propagated from\n",
    "                color_diff = torch.abs(color_map - torch.roll(color_map, shifts=(dy, dx), dims=(2, 3))).sum(dim=1)  # Sum over color channels\n",
    "\n",
    "                # Add the gradient map value as a weighted penalty to the distance\n",
    "                weighted_dist = shifted_dist + weighted_grad_map[:, 0, :, :] + color_diff * color_weight\n",
    "                \n",
    "                # Update the mask and distance map where the new combined distance is smaller\n",
    "                update_mask = weighted_dist < dist_map\n",
    "                dist_map[update_mask] = weighted_dist[update_mask]\n",
    "                mask[update_mask] = shifted_mask[update_mask]\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C_in, H, W = x.shape\n",
    "        \n",
    "        if C_in == 3:\n",
    "            grayscale_image = self.convert_to_greyscale(x)\n",
    "        else:\n",
    "            grayscale_image = x\n",
    "        \n",
    "        # Compute the gradient map from grayscale image\n",
    "        grad_map = self.compute_gradient_map(grayscale_image)\n",
    "        \n",
    "        # Place centroids on a grid\n",
    "        centroids = self.place_centroids_on_grid(B)\n",
    "        \n",
    "        # Move centroids to nearest local minima\n",
    "        centroids = self.find_nearest_minima(centroids, grad_map)\n",
    "        \n",
    "        # Use the color map (the original image) to guide propagation\n",
    "        spixel_features = self.unet(x)\n",
    "        \n",
    "        # Perform distance-weighted propagation with both gradient and color guidance\n",
    "        mask = self.distance_weighted_propagation(centroids, grad_map, spixel_features)\n",
    "        \n",
    "        # return grad_map, centroids, mask, spixel_features\n",
    "        return grad_map, centroids, mask, spixel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundaryPathFinder(nn.Module):\n",
    "    def __init__(self, num_segments_row=8, num_segments_col=8, height=224, width=224, device='cpu'):\n",
    "        super(BoundaryPathFinder, self).__init__()\n",
    "        \n",
    "        self.num_segments_row = num_segments_row\n",
    "        self.num_segments_col = num_segments_col\n",
    "        self.H = height\n",
    "        self.W = width\n",
    "        self.device = device\n",
    "        \n",
    "        self.convert_to_grayscale = torchvision.transforms.Grayscale(num_output_channels=1)\n",
    "    \n",
    "    def compute_gradient_map(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        if x.shape[1] == 3:\n",
    "            x = self.convert_to_grayscale(x)\n",
    "        \n",
    "        # Sobel kernels\n",
    "        sobel_x = torch.tensor([[[[-1, 0, 1], \n",
    "                                  [-2, 0, 2], \n",
    "                                  [-1, 0, 1]]]], device=x.device, dtype=x.dtype)\n",
    "        sobel_y = torch.tensor([[[[-1, -2, -1], \n",
    "                                  [0, 0, 0], \n",
    "                                  [1, 2, 1]]]], device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        # Apply Sobel filters\n",
    "        grad_x = F.conv2d(x, sobel_x, padding=1)\n",
    "        grad_y = F.conv2d(x, sobel_y, padding=1)\n",
    "\n",
    "        # Compute gradient magnitude\n",
    "        grad_map = torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-8)\n",
    "        return grad_map  # Shape: (B, 1, H, W)\n",
    "    \n",
    "    def initialize_grid(self, batch_size):\n",
    "        # Create initial grid segmentation\n",
    "        # Output shape: (B, H, W)\n",
    "        segment_height = self.H // self.num_segments_row\n",
    "        segment_width = self.W // self.num_segments_col\n",
    "\n",
    "        # Create grid labels\n",
    "        rows = torch.arange(self.H, device=self.device).unsqueeze(1).expand(self.H, self.W)\n",
    "        cols = torch.arange(self.W, device=self.device).unsqueeze(0).expand(self.H, self.W)\n",
    "\n",
    "        row_labels = rows // segment_height\n",
    "        col_labels = cols // segment_width\n",
    "\n",
    "        labels = (row_labels * self.num_segments_col + col_labels).to(torch.int32)\n",
    "        labels = labels.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: (B, H, W)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def adjust_boundaries(self, grad_map, segmentation_mask, band_width=5):\n",
    "        \"\"\"\n",
    "        Adjust boundary lines to align with the highest gradients while keeping the number of segments constant.\n",
    "        \"\"\"\n",
    "        B, H, W = segmentation_mask.shape\n",
    "        device = grad_map.device\n",
    "\n",
    "        # Prepare indices\n",
    "        y_indices = torch.arange(H, device=device)\n",
    "        x_indices = torch.arange(W, device=device)\n",
    "\n",
    "        # Initialize boundary masks\n",
    "        boundary_masks = torch.zeros((B, H, W), dtype=torch.bool, device=device)\n",
    "\n",
    "        # Process each image in the batch\n",
    "        for b in range(B):\n",
    "            grad_map_b = grad_map[b, 0]  # Shape: (H, W)\n",
    "\n",
    "            # Vertical boundaries\n",
    "            vertical_paths = []\n",
    "            for i in range(1, self.num_segments_col):\n",
    "                x_init = i * (W // self.num_segments_col)\n",
    "                x_init = min(x_init, W - 1)\n",
    "                path = self.find_optimal_vertical_path(grad_map_b, x_init, band_width)\n",
    "                vertical_paths.append(path)\n",
    "\n",
    "            # Mark vertical boundaries\n",
    "            for path in vertical_paths:\n",
    "                boundary_masks[b, y_indices, path] = True\n",
    "\n",
    "            # Horizontal boundaries\n",
    "            horizontal_paths = []\n",
    "            for i in range(1, self.num_segments_row):\n",
    "                y_init = i * (H // self.num_segments_row)\n",
    "                y_init = min(y_init, H - 1)\n",
    "                path = self.find_optimal_horizontal_path(grad_map_b, y_init, band_width)\n",
    "                horizontal_paths.append(path)\n",
    "\n",
    "            # Mark horizontal boundaries\n",
    "            for path in horizontal_paths:\n",
    "                boundary_masks[b, path, x_indices] = True\n",
    "\n",
    "        # Use connected components labeling and reassign labels based on majority voting\n",
    "        from skimage.measure import label as skimage_label\n",
    "\n",
    "        new_segmentation_masks = []\n",
    "        for b in range(B):\n",
    "            boundary_mask_np = boundary_masks[b].cpu().numpy()\n",
    "            regions = ~boundary_mask_np\n",
    "            labeled_array = skimage_label(regions, connectivity=1)\n",
    "            initial_labels = segmentation_mask[b].cpu().numpy()\n",
    "            new_segmentation_mask = np.zeros_like(labeled_array, dtype=initial_labels.dtype)\n",
    "            for region_label in np.unique(labeled_array):\n",
    "                if region_label == 0:\n",
    "                    continue  # Skip background if any\n",
    "                region_mask = (labeled_array == region_label)\n",
    "                labels_in_region = initial_labels[region_mask]\n",
    "                if labels_in_region.size == 0:\n",
    "                    continue\n",
    "                # Majority voting to find the most common label\n",
    "                majority_label = np.bincount(labels_in_region).argmax()\n",
    "                new_segmentation_mask[region_mask] = majority_label\n",
    "            new_segmentation_masks.append(torch.from_numpy(new_segmentation_mask).to(device))\n",
    "\n",
    "        new_segmentation_masks = torch.stack(new_segmentation_masks, dim=0).to(torch.int32)\n",
    "\n",
    "        return new_segmentation_masks  # Shape: (B, H, W)\n",
    "    \n",
    "    def find_optimal_vertical_path(self, grad_map, x_init, band_width):\n",
    "        H, W = grad_map.shape\n",
    "        device = grad_map.device\n",
    "\n",
    "        # Define band around x_init\n",
    "        x_indices = x_init + torch.arange(-band_width, band_width + 1, device=device)\n",
    "        x_indices = x_indices.clamp(0, W - 1)\n",
    "        num_positions = x_indices.size(0)\n",
    "\n",
    "        # Initialize cost and path matrices\n",
    "        cost = torch.full((H, num_positions), float('inf'), device=device)\n",
    "        path = torch.zeros((H, num_positions), dtype=torch.long, device=device)\n",
    "\n",
    "        # First row\n",
    "        cost[0] = -grad_map[0, x_indices]\n",
    "\n",
    "        # Dynamic programming\n",
    "        for y in range(1, H):\n",
    "            for i in range(num_positions):\n",
    "                prev_costs = []\n",
    "                prev_indices = []\n",
    "                # Stay\n",
    "                prev_costs.append(cost[y - 1, i])\n",
    "                prev_indices.append(i)\n",
    "                # Left\n",
    "                if i > 0:\n",
    "                    prev_costs.append(cost[y - 1, i - 1])\n",
    "                    prev_indices.append(i - 1)\n",
    "                else:\n",
    "                    prev_costs.append(float('inf'))\n",
    "                    prev_indices.append(i)\n",
    "                # Right\n",
    "                if i < num_positions - 1:\n",
    "                    prev_costs.append(cost[y - 1, i + 1])\n",
    "                    prev_indices.append(i + 1)\n",
    "                else:\n",
    "                    prev_costs.append(float('inf'))\n",
    "                    prev_indices.append(i)\n",
    "\n",
    "                min_prev_cost = min(prev_costs)\n",
    "                min_prev_idx = prev_indices[prev_costs.index(min_prev_cost)]\n",
    "                cost[y, i] = min_prev_cost - grad_map[y, x_indices[i]]\n",
    "                path[y, i] = min_prev_idx\n",
    "\n",
    "        # Backtracking\n",
    "        idx = cost[-1].argmin().item()\n",
    "        optimal_path = []\n",
    "        for y in reversed(range(H)):\n",
    "            optimal_path.append(x_indices[idx])\n",
    "            idx = path[y, idx].item()\n",
    "        optimal_path.reverse()\n",
    "        optimal_path = torch.stack(optimal_path)\n",
    "        return optimal_path  # Shape: (H,)\n",
    "        \n",
    "    def find_optimal_horizontal_path(self, grad_map, y_init, band_width):\n",
    "        \"\"\"\n",
    "        Find optimal horizontal path around the initial y position.\n",
    "        \"\"\"\n",
    "        H, W = grad_map.shape\n",
    "        device = grad_map.device\n",
    "\n",
    "        # Define band around y_init\n",
    "        y_indices = y_init + torch.arange(-band_width, band_width + 1, device=device)\n",
    "        y_indices = y_indices.clamp(0, H - 1)\n",
    "        num_positions = y_indices.size(0)\n",
    "\n",
    "        # Initialize cost and path matrices\n",
    "        cost = torch.full((W, num_positions), float('inf'), device=device)\n",
    "        path = torch.zeros((W, num_positions), dtype=torch.long, device=device)\n",
    "\n",
    "        # First column\n",
    "        cost[0] = -grad_map[y_indices, 0]\n",
    "\n",
    "        # Dynamic programming\n",
    "        for x in range(1, W):\n",
    "            prev_cost = cost[x - 1]\n",
    "            prev_indices = torch.stack([\n",
    "                torch.arange(num_positions, device=device).clamp(0, num_positions - 1),\n",
    "                (torch.arange(num_positions, device=device) - 1).clamp(0, num_positions - 1),\n",
    "                (torch.arange(num_positions, device=device) + 1).clamp(0, num_positions - 1)\n",
    "            ], dim=1)\n",
    "            prev_costs = prev_cost[prev_indices]\n",
    "            min_prev_costs, min_indices = prev_costs.min(dim=1)\n",
    "            cost[x] = min_prev_costs - grad_map[y_indices, x]\n",
    "            path[x] = prev_indices[torch.arange(num_positions, device=device), min_indices]\n",
    "\n",
    "        # Backtracking\n",
    "        idx = cost[-1].argmin().item()\n",
    "        optimal_path = []\n",
    "        for x in reversed(range(W)):\n",
    "            optimal_path.append(y_indices[idx])\n",
    "            idx = path[x, idx].item()\n",
    "        optimal_path.reverse()\n",
    "        optimal_path = torch.stack(optimal_path)\n",
    "        return optimal_path  # Shape: (W,)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        if H != self.H or W != self.W:\n",
    "            raise ValueError(f\"Input image size must match initialized size: ({self.H}, {self.W})\")\n",
    "\n",
    "        # Compute gradient map\n",
    "        grad_map = self.compute_gradient_map(x)  # Shape: (B, 1, H, W)\n",
    "\n",
    "        # Initialize grid segmentation\n",
    "        segmentation_mask = self.initialize_grid(B)  # Shape: (B, H, W)\n",
    "\n",
    "        # Adjust boundaries\n",
    "        new_segmentation_mask = self.adjust_boundaries(grad_map, segmentation_mask)\n",
    "\n",
    "        return grad_map, segmentation_mask, new_segmentation_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "voronoi_model = VoronoiPropagation(num_clusters=256)\n",
    "pathFinder_model = BoundaryPathFinder(num_segments_col=16,num_segments_row=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explained_variance_batch(image_batch, superpixel_labels_batch):\n",
    "    batch_size, num_channels, height, width = image_batch.shape\n",
    "    explained_variance_scores = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image = image_batch[i]  # Shape: (C, H, W)\n",
    "        superpixel_labels = superpixel_labels_batch[i]  # Shape: (H, W)\n",
    "\n",
    "        # Ensure superpixel_labels is in shape (H, W)\n",
    "        superpixel_labels = superpixel_labels.squeeze().to(image.device)\n",
    "\n",
    "        # Flatten image and labels for computation\n",
    "        image_flat = image.view(num_channels, height * width)\n",
    "        labels_flat = superpixel_labels.view(height * width)\n",
    "\n",
    "        # Compute total variance of the image across all channels\n",
    "        total_variance = image_flat.var(dim=1, unbiased=False).mean().item()\n",
    "\n",
    "        # Proceed to compute within-superpixel variance (homogeneity_score)\n",
    "        unique_labels = superpixel_labels.unique()\n",
    "        num_superpixels = unique_labels.size(0)\n",
    "\n",
    "        pixel_sums = torch.zeros((num_superpixels, num_channels), device=image.device)\n",
    "        pixel_squares = torch.zeros((num_superpixels, num_channels), device=image.device)\n",
    "        pixel_counts = torch.zeros(num_superpixels, device=image.device)\n",
    "\n",
    "        for j, label in enumerate(unique_labels):\n",
    "            mask = (labels_flat == label)\n",
    "            pixel_sums[j] = image_flat[:, mask].sum(dim=1)\n",
    "            pixel_squares[j] = (image_flat[:, mask] ** 2).sum(dim=1)\n",
    "            pixel_counts[j] = mask.sum()\n",
    "\n",
    "        pixel_means = pixel_sums / pixel_counts.unsqueeze(1)\n",
    "        pixel_variances = (pixel_squares / pixel_counts.unsqueeze(1)) - (pixel_means ** 2)\n",
    "        within_variance = pixel_variances.mean().item()\n",
    "\n",
    "        # Compute explained variance\n",
    "        explained_variance = 1 - (within_variance / total_variance)\n",
    "        explained_variance_scores.append(explained_variance)\n",
    "\n",
    "    return explained_variance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voronoi took 0.5320451259613037\n",
      "Pathfinder took 3.578075647354126\n",
      "Voronoi took 0.47411561012268066\n",
      "Pathfinder took 3.549269437789917\n",
      "Voronoi took 0.45554018020629883\n",
      "Pathfinder took 3.525947332382202\n",
      "Voronoi took 0.4609999656677246\n",
      "Pathfinder took 3.550792932510376\n",
      "Voronoi took 0.5000004768371582\n",
      "Pathfinder took 3.5789008140563965\n",
      "Voronoi took 0.4665405750274658\n",
      "Pathfinder took 3.4533157348632812\n",
      "Voronoi took 0.4559206962585449\n",
      "Pathfinder took 3.4973716735839844\n",
      "Voronoi took 0.4789726734161377\n",
      "Pathfinder took 3.556828498840332\n",
      "Voronoi took 0.4499983787536621\n",
      "Pathfinder took 3.4982306957244873\n",
      "Voronoi took 0.45099759101867676\n",
      "Pathfinder took 3.501106023788452\n",
      "Voronoi took 0.46323561668395996\n",
      "Pathfinder took 3.529568672180176\n",
      "Voronoi took 0.4738783836364746\n",
      "Pathfinder took 3.515782117843628\n",
      "Voronoi took 0.45812439918518066\n",
      "Pathfinder took 3.4899282455444336\n",
      "Voronoi took 0.4939994812011719\n",
      "Pathfinder took 3.5049455165863037\n",
      "Voronoi took 0.43920445442199707\n",
      "Pathfinder took 3.6039679050445557\n",
      "Voronoi took 0.46456193923950195\n",
      "Pathfinder took 3.4945476055145264\n",
      "Voronoi took 0.44753479957580566\n",
      "Pathfinder took 3.6650032997131348\n",
      "Voronoi took 0.4570021629333496\n",
      "Pathfinder took 3.520946979522705\n",
      "Voronoi took 0.47914671897888184\n",
      "Pathfinder took 3.6930038928985596\n",
      "Voronoi took 0.6231405735015869\n",
      "Pathfinder took 3.389286994934082\n",
      "Voronoi took 0.4250006675720215\n",
      "Pathfinder took 3.33475923538208\n",
      "Voronoi took 0.40200042724609375\n",
      "Pathfinder took 3.4737141132354736\n",
      "Voronoi took 0.48799967765808105\n",
      "Pathfinder took 3.3659470081329346\n",
      "Voronoi took 0.4400007724761963\n",
      "Pathfinder took 3.307021379470825\n",
      "Voronoi took 0.42600321769714355\n",
      "Pathfinder took 3.2880921363830566\n",
      "Voronoi took 0.4199976921081543\n",
      "Pathfinder took 3.299281597137451\n",
      "Voronoi took 0.40800046920776367\n",
      "Pathfinder took 3.306457281112671\n",
      "Voronoi took 0.42460203170776367\n",
      "Pathfinder took 3.2701382637023926\n",
      "Voronoi took 0.39359188079833984\n",
      "Pathfinder took 3.2961668968200684\n",
      "Voronoi took 0.4824221134185791\n",
      "Pathfinder took 3.5774924755096436\n",
      "Voronoi took 0.423999547958374\n",
      "Pathfinder took 3.618558168411255\n",
      "Voronoi took 0.4650003910064697\n",
      "Pathfinder took 3.43053936958313\n",
      "Voronoi took 0.44069552421569824\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "explained_variance_scores_voronoi = []\n",
    "explained_variance_scores_pathfinder = []\n",
    "\n",
    "for (image, labels) in dataloader:\n",
    "    start = time.time()\n",
    "    voronor_grad_map, voronor_centroids, voronor_mask, voronor_spixel_features = voronoi_model(image)\n",
    "    end = time.time()\n",
    "    print(\"Voronoi took\", end-start)\n",
    "    start = time.time()\n",
    "    pathfinder_grad_map, pathfinder_segmentation_mask, pathfinder_new_segmentation_mask = pathFinder_model(image)\n",
    "    end = time.time()\n",
    "    print(\"Pathfinder took\", end-start)\n",
    "    \n",
    "    explained_variance_scores_voronoi.append(explained_variance_batch(image, voronor_mask))\n",
    "    explained_variance_scores_pathfinder.append(explained_variance_batch(image, pathfinder_new_segmentation_mask))\n",
    "    \n",
    "\n",
    "print(np.mean(np.array(explained_variance_scores_voronoi)))\n",
    "print(np.mean(np.array(explained_variance_scores_pathfinder)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
