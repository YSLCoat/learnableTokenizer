{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import thin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "\n",
    "\n",
    "cmap = mpl.colors.ListedColormap(torch.rand(256**2, 3).numpy())\n",
    "\n",
    "def plot_segmentation_boundaries(image_np, output_mask, figsize=(15, 15)):\n",
    "    \"\"\"\n",
    "    Plot the boundaries of the segmentation over the original image.\n",
    "    Uses pixel comparison to detect boundaries and applies thinning.\n",
    "    \"\"\"\n",
    "    output_mask_np = output_mask[0].cpu().numpy()\n",
    "\n",
    "    # Find boundaries by comparing neighboring pixels\n",
    "    boundaries = np.zeros_like(output_mask_np)\n",
    "    boundaries[1:, :] = np.logical_or(boundaries[1:, :], output_mask_np[1:, :] != output_mask_np[:-1, :])  # Compare vertically\n",
    "    boundaries[:, 1:] = np.logical_or(boundaries[:, 1:], output_mask_np[:, 1:] != output_mask_np[:, :-1])  # Compare horizontally\n",
    "\n",
    "    # Apply thinning to ensure boundaries are only 1 pixel wide\n",
    "    boundaries = thin(boundaries)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    ax.imshow(image_np)\n",
    "\n",
    "    ax.contour(boundaries, colors='red', linewidths=0.7)\n",
    "\n",
    "    ax.set_title('Image with Segmentation Boundaries (Thinned)')\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_mask(mask):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask[0].cpu().numpy(), cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    plt.title('Superpixel Mask')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_gradient_map(grad_map):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grad_map[0, 0].cpu().numpy(), cmap='gray')\n",
    "    plt.colorbar()\n",
    "    plt.title('Gradient Map')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "class BSDS500Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.images_dir = os.path.join(root_dir, 'images', split)\n",
    "        self.ground_truth_dir = os.path.join(root_dir, 'ground_truth', split)\n",
    "        self.image_files = [f for f in os.listdir(self.images_dir) if f.endswith('.jpg')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        gt_name = os.path.join(self.ground_truth_dir, self.image_files[idx].replace('.jpg', '.mat'))\n",
    "        gt_data = sio.loadmat(gt_name)\n",
    "        ground_truth = gt_data['groundTruth'][0][0][0][0][1]\n",
    "\n",
    "        \n",
    "        #print(ground_truth)\n",
    "        # print(ground_truth[0, 0])\n",
    "        # print(ground_truth[0, 0]['Segmentation'])\n",
    "        segmentation = ground_truth\n",
    "        \n",
    "        if isinstance(segmentation, np.ndarray) and segmentation.shape == (1, 1):\n",
    "            segmentation = segmentation[0, 0]\n",
    "        \n",
    "        segmentation = Image.fromarray(segmentation)\n",
    "        segmentation = segmentation.resize((224, 224), Image.NEAREST)\n",
    "        \n",
    "        segmentation = np.array(segmentation, dtype=np.int64)\n",
    "\n",
    "        segmentation = torch.tensor(segmentation, dtype=torch.long)\n",
    "        \n",
    "        return image, segmentation\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
    "])\n",
    "\n",
    "dataset_train = BSDS500Dataset(root_dir=r'D:\\Data\\BSDS500\\data', split='train', transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset_train, batch_size=10, shuffle=True, num_workers=0)\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import segmentation_models_pytorch as smp\n",
    "import math\n",
    "\n",
    "\n",
    "class VoronoiPropagation(nn.Module):\n",
    "    def __init__(self, num_clusters=64, n_channels=3, height=224, width=224, device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_clusters (int): Number of clusters (centroids) to initialize.\n",
    "            height (int): Height of the input image.\n",
    "            width (int): Width of the input image.\n",
    "            device (str): Device to run the model ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        super(VoronoiPropagation, self).__init__()\n",
    "        \n",
    "        self.C = num_clusters\n",
    "        self.H = height\n",
    "        self.W = width\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        self.unet = smp.Unet(encoder_name=\"efficientnet-b0\",\n",
    "                             encoder_weights=None,  \n",
    "                             in_channels=n_channels,               \n",
    "                             classes=n_channels)   \n",
    "        \n",
    "        # Set bandwidth / sigma for kernel\n",
    "        self.std = self.C / (self.H * self.W)**0.5\n",
    "        \n",
    "        self.convert_to_greyscale = torchvision.transforms.Grayscale(num_output_channels=1)\n",
    "\n",
    "    def compute_gradient_map(self, x):\n",
    "        # Sobel kernels for single-channel input\n",
    "        sobel_x = torch.tensor([[[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]]], device=x.device, dtype=x.dtype)\n",
    "        sobel_y = torch.tensor([[[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]]], device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        # Apply Sobel filters\n",
    "        grad_x = F.conv2d(x, sobel_x, padding=1)\n",
    "        grad_y = F.conv2d(x, sobel_y, padding=1)\n",
    "        \n",
    "        # Compute gradient magnitude\n",
    "        grad_map = torch.sqrt(grad_x.pow(2) + grad_y.pow(2))\n",
    "        return grad_map\n",
    "\n",
    "    def place_centroids_on_grid(self, batch_size):\n",
    "        num_cols = int(math.sqrt(self.C * self.W / self.H))\n",
    "        num_rows = int(math.ceil(self.C / num_cols))\n",
    "\n",
    "        grid_spacing_y = self.H / num_rows\n",
    "        grid_spacing_x = self.W / num_cols\n",
    "\n",
    "        centroids = []\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_cols):\n",
    "                if len(centroids) >= self.C:\n",
    "                    break\n",
    "                y = int((i + 0.5) * grid_spacing_y)\n",
    "                x = int((j + 0.5) * grid_spacing_x)\n",
    "                centroids.append([y, x])\n",
    "            if len(centroids) >= self.C:\n",
    "                break\n",
    "\n",
    "        centroids = torch.tensor(centroids, device=self.device).float()\n",
    "        return centroids.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "    def find_nearest_minima(self, centroids, grad_map, neighborhood_size=10):\n",
    "        updated_centroids = []\n",
    "        B, _, _ = centroids.shape\n",
    "        \n",
    "        for batch_idx in range(B):\n",
    "            updated_centroids_batch = []\n",
    "            occupied_positions = set()\n",
    "            for centroid in centroids[batch_idx]:\n",
    "                y, x = centroid\n",
    "                y_min = max(0, int(y) - neighborhood_size)\n",
    "                y_max = min(self.H, int(y) + neighborhood_size)\n",
    "                x_min = max(0, int(x) - neighborhood_size)\n",
    "                x_max = min(self.W, int(x) + neighborhood_size)\n",
    "                \n",
    "                neighborhood = grad_map[batch_idx, 0, y_min:y_max, x_min:x_max]\n",
    "                min_val = torch.min(neighborhood)\n",
    "                min_coords = torch.nonzero(neighborhood == min_val, as_tuple=False)\n",
    "                \n",
    "                # Iterate over all minima to find an unoccupied one\n",
    "                found = False\n",
    "                for coord in min_coords:\n",
    "                    new_y = y_min + coord[0].item()\n",
    "                    new_x = x_min + coord[1].item()\n",
    "                    position = (new_y, new_x)\n",
    "                    if position not in occupied_positions:\n",
    "                        occupied_positions.add(position)\n",
    "                        updated_centroids_batch.append([new_y, new_x])\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    # If all minima are occupied, keep the original position\n",
    "                    updated_centroids_batch.append([y.item(), x.item()])\n",
    "            \n",
    "            updated_centroids.append(torch.tensor(updated_centroids_batch, device=self.device))\n",
    "        \n",
    "        return torch.stack(updated_centroids, dim=0)\n",
    "\n",
    "    def distance_weighted_propagation(self, centroids, grad_map, color_map, num_iters=50, gradient_weight=10.0, color_weight=10.0, edge_exponent=4.0): # gradient weight, color weight and edge exponent are all tuneable parameters \n",
    "        \"\"\"\n",
    "        Perform Voronoi-like propagation from centroids, guided by both the gradient map and color similarity.\n",
    "        \n",
    "        Args:\n",
    "            centroids (Tensor): Initial centroid positions.\n",
    "            grad_map (Tensor): Gradient magnitude map.\n",
    "            color_map (Tensor): Input image for color similarity.\n",
    "            num_iters (int): Number of iterations to perform propagation.\n",
    "            gradient_weight (float): Weight for the gradient penalty.\n",
    "            color_weight (float): Weight for the color similarity penalty.\n",
    "            edge_exponent (float): Exponent to amplify edge gradients.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Final segmentation mask.\n",
    "        \"\"\"\n",
    "        B, _, H, W = grad_map.shape\n",
    "        mask = torch.full((B, H, W), fill_value=-1, device=grad_map.device)  # Label mask\n",
    "        dist_map = torch.full((B, H, W), fill_value=float('inf'), device=grad_map.device)  # Distance map\n",
    "        \n",
    "        for batch_idx in range(B):\n",
    "            for idx, (cy, cx) in enumerate(centroids[batch_idx]):\n",
    "                mask[batch_idx, int(cy), int(cx)] = idx\n",
    "                dist_map[batch_idx, int(cy), int(cx)] = 0  # Distance from centroid is 0 initially\n",
    "        \n",
    "        # 4-connected neighbors (dy, dx)\n",
    "        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "        \n",
    "        # Amplify the impact of the gradient map by multiplying it with a weight and applying a non-linear transformation\n",
    "        weighted_grad_map = (grad_map ** edge_exponent) * gradient_weight\n",
    "\n",
    "        # Perform propagation with both gradient penalties and color similarity\n",
    "        for _ in range(num_iters):\n",
    "            prev_dist_map = dist_map.clone()\n",
    "            # Perform propagation...\n",
    "            if torch.allclose(prev_dist_map, dist_map, atol=1e-3):\n",
    "                break\n",
    "            for dy, dx in directions:\n",
    "                # Shift the distance map in each direction\n",
    "                shifted_dist = torch.roll(dist_map, shifts=(dy, dx), dims=(1, 2))\n",
    "                shifted_mask = torch.roll(mask, shifts=(dy, dx), dims=(1, 2))\n",
    "                \n",
    "                # Calculate color distance between current pixel and centroid it is being propagated from\n",
    "                color_diff = torch.abs(color_map - torch.roll(color_map, shifts=(dy, dx), dims=(2, 3))).sum(dim=1)  # Sum over color channels\n",
    "\n",
    "                # Add the gradient map value as a weighted penalty to the distance\n",
    "                weighted_dist = shifted_dist + weighted_grad_map[:, 0, :, :] + color_diff * color_weight\n",
    "                \n",
    "                # Update the mask and distance map where the new combined distance is smaller\n",
    "                update_mask = weighted_dist < dist_map\n",
    "                dist_map[update_mask] = weighted_dist[update_mask]\n",
    "                mask[update_mask] = shifted_mask[update_mask]\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C_in, H, W = x.shape\n",
    "        \n",
    "        if C_in == 3:\n",
    "            grayscale_image = self.convert_to_greyscale(x)\n",
    "        else:\n",
    "            grayscale_image = x\n",
    "        \n",
    "        # Compute the gradient map from grayscale image\n",
    "        grad_map = self.compute_gradient_map(grayscale_image)\n",
    "        \n",
    "        # Place centroids on a grid\n",
    "        centroids = self.place_centroids_on_grid(B)\n",
    "        \n",
    "        # Move centroids to nearest local minima\n",
    "        centroids = self.find_nearest_minima(centroids, grad_map)\n",
    "        \n",
    "        # Use the color map (the original image) to guide propagation\n",
    "        #spixel_features = self.unet(x)\n",
    "        \n",
    "        # Perform distance-weighted propagation with both gradient and color guidance\n",
    "        mask = self.distance_weighted_propagation(centroids, grad_map, x)\n",
    "        \n",
    "        # return grad_map, centroids, mask, spixel_features\n",
    "        return grad_map, centroids, mask, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundaryPathFinder2(nn.Module):\n",
    "    def __init__(self, num_segments_row=8, num_segments_col=8, height=224, width=224, device='cpu'):\n",
    "        super(BoundaryPathFinder2, self).__init__()\n",
    "        \n",
    "        self.num_segments_row = num_segments_row\n",
    "        self.num_segments_col = num_segments_col\n",
    "        self.H = height\n",
    "        self.W = width\n",
    "        self.device = device\n",
    "        \n",
    "        self.convert_to_grayscale = torchvision.transforms.Grayscale(num_output_channels=1)\n",
    "        \n",
    "        # Sobel kernels\n",
    "        self.sobel_x = torch.tensor([[[[-1, 0, 1], \n",
    "                                  [-2, 0, 2], \n",
    "                                  [-1, 0, 1]]]], device=device, dtype=torch.float32)\n",
    "        self.sobel_y = torch.tensor([[[[-1, -2, -1], \n",
    "                                  [0, 0, 0], \n",
    "                                  [1, 2, 1]]]], device=device, dtype=torch.float32)\n",
    "        \n",
    "        # Move offsets for dynamic programming\n",
    "        self.move_offsets = torch.tensor([-1, 0, 1], device=device)\n",
    "    \n",
    "    def compute_gradient_map(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        if x.shape[1] == 3:\n",
    "            x = self.convert_to_grayscale(x)\n",
    "        \n",
    "        # Apply Sobel filters\n",
    "        grad_x = F.conv2d(x, self.sobel_x, padding=1)\n",
    "        grad_y = F.conv2d(x, self.sobel_y, padding=1)\n",
    "\n",
    "        # Compute gradient magnitude\n",
    "        grad_map = torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-8)\n",
    "        return grad_map  # Shape: (B, 1, H, W)\n",
    "    \n",
    "    def initialize_grid(self, batch_size):\n",
    "        # Create grid labels\n",
    "        rows = torch.arange(self.H, device=self.device).unsqueeze(1)\n",
    "        cols = torch.arange(self.W, device=self.device).unsqueeze(0)\n",
    "\n",
    "        row_labels = rows // (self.H // self.num_segments_row)\n",
    "        col_labels = cols // (self.W // self.num_segments_col)\n",
    "\n",
    "        labels = (row_labels * self.num_segments_col + col_labels).to(torch.int32)\n",
    "        labels = labels.expand(batch_size, -1, -1)  # Shape: (B, H, W)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def adjust_boundaries(self, grad_map, segmentation_mask, band_width=5):\n",
    "        \"\"\"\n",
    "        Adjust boundary lines to align with the highest gradients while keeping the number of segments constant.\n",
    "        \"\"\"\n",
    "        B, H, W = segmentation_mask.shape\n",
    "        device = grad_map.device\n",
    "\n",
    "        # Prepare indices\n",
    "        y_indices = torch.arange(H, device=device)\n",
    "        x_indices = torch.arange(W, device=device)\n",
    "\n",
    "        # Initialize boundary masks\n",
    "        boundary_masks_vertical = torch.zeros((B, H, W), dtype=torch.bool, device=device)\n",
    "        boundary_masks_horizontal = torch.zeros((B, H, W), dtype=torch.bool, device=device)\n",
    "\n",
    "        # Vertical boundaries\n",
    "        x_inits = torch.tensor([i * (W // self.num_segments_col) for i in range(1, self.num_segments_col)], device=device).clamp(0, W - 1)\n",
    "        num_vertical_paths = x_inits.size(0)\n",
    "        for b in range(B):\n",
    "            grad_map_b = grad_map[b, 0]  # Shape: (H, W)\n",
    "            vertical_paths = self.find_optimal_vertical_paths(grad_map_b, x_inits, band_width)  # Shape: (num_vertical_paths, H)\n",
    "            # Mark vertical boundaries\n",
    "            for i in range(num_vertical_paths):\n",
    "                boundary_masks_vertical[b, y_indices, vertical_paths[i]] = True\n",
    "\n",
    "        # Horizontal boundaries\n",
    "        y_inits = torch.tensor([i * (H // self.num_segments_row) for i in range(1, self.num_segments_row)], device=device).clamp(0, H - 1)\n",
    "        num_horizontal_paths = y_inits.size(0)\n",
    "        for b in range(B):\n",
    "            grad_map_b = grad_map[b, 0]  # Shape: (H, W)\n",
    "            horizontal_paths = self.find_optimal_horizontal_paths(grad_map_b, y_inits, band_width)  # Shape: (num_horizontal_paths, W)\n",
    "            # Mark horizontal boundaries\n",
    "            for i in range(num_horizontal_paths):\n",
    "                boundary_masks_horizontal[b, horizontal_paths[i], x_indices] = True\n",
    "\n",
    "        # Compute vertical labels\n",
    "        vertical_boundaries_int = boundary_masks_vertical.to(torch.int32)\n",
    "        vertical_labels = torch.cumsum(vertical_boundaries_int, dim=2)\n",
    "\n",
    "        # Compute horizontal labels\n",
    "        horizontal_boundaries_int = boundary_masks_horizontal.to(torch.int32)\n",
    "        horizontal_labels = torch.cumsum(horizontal_boundaries_int, dim=1)\n",
    "\n",
    "        # Compute final region labels\n",
    "        num_vertical_segments = self.num_segments_col\n",
    "        num_horizontal_segments = self.num_segments_row\n",
    "\n",
    "        new_segmentation_masks = vertical_labels + num_vertical_segments * horizontal_labels\n",
    "\n",
    "        return new_segmentation_masks  # Shape: (B, H, W)\n",
    "\n",
    "    def find_optimal_vertical_paths(self, grad_map, x_inits, band_width):\n",
    "        \"\"\"\n",
    "        Find the optimal vertical paths around the initial x positions using dynamic programming.\n",
    "        \"\"\"\n",
    "        H, W = grad_map.shape\n",
    "        device = grad_map.device\n",
    "        num_paths = x_inits.size(0)\n",
    "\n",
    "        # Define bands around x_inits\n",
    "        x_offsets = torch.arange(-band_width, band_width + 1, device=device)\n",
    "        x_indices = x_inits.unsqueeze(1) + x_offsets.unsqueeze(0)  # Shape: (num_paths, num_positions)\n",
    "        x_indices = x_indices.clamp(0, W - 1).long()\n",
    "        num_positions = x_indices.size(1)\n",
    "\n",
    "        # Initialize cost and path matrices\n",
    "        cost = torch.full((H, num_paths, num_positions), float('inf'), device=device)\n",
    "        path = torch.zeros((H, num_paths, num_positions), dtype=torch.long, device=device)\n",
    "\n",
    "        # First row\n",
    "        grad_row = grad_map[0].unsqueeze(0).expand(num_paths, -1)  # Shape: (num_paths, W)\n",
    "        cost[0] = -grad_row.gather(1, x_indices)  # Shape: (num_paths, num_positions)\n",
    "\n",
    "        # Precompute position indices\n",
    "        positions = torch.arange(num_positions, device=device).unsqueeze(0).expand(num_paths, -1)  # Shape: (num_paths, num_positions)\n",
    "\n",
    "        # Dynamic programming\n",
    "        for y in range(1, H):\n",
    "            # Pad the previous cost for easy indexing\n",
    "            padded_prev_cost = torch.cat([\n",
    "                torch.full((num_paths, 1), float('inf'), device=device),\n",
    "                cost[y - 1],\n",
    "                torch.full((num_paths, 1), float('inf'), device=device)\n",
    "            ], dim=1)  # Shape: (num_paths, num_positions + 2)\n",
    "\n",
    "            # Indices for possible moves: left (-1), stay (0), right (+1)\n",
    "            move_offsets = torch.tensor([-1, 0, 1], device=device)\n",
    "            neighbor_indices = positions.unsqueeze(2) + move_offsets.view(1, 1, -1)  # Shape: (num_paths, num_positions, 3)\n",
    "            neighbor_indices = neighbor_indices.clamp(0, num_positions - 1)\n",
    "\n",
    "            # Adjust for padding\n",
    "            neighbor_indices_padded = neighbor_indices + 1  # Adjust for the padding\n",
    "            neighbor_indices_padded = neighbor_indices_padded.long()\n",
    "\n",
    "            # Adjust dimensions of padded_prev_cost\n",
    "            padded_prev_cost_expanded = padded_prev_cost.unsqueeze(1).expand(-1, num_positions, -1)\n",
    "\n",
    "            # Gather costs for possible moves\n",
    "            prev_costs = padded_prev_cost_expanded.gather(2, neighbor_indices_padded)  # Shape: (num_paths, num_positions, 3)\n",
    "\n",
    "            # Find the minimum cost among the neighbors\n",
    "            min_prev_costs, min_indices = prev_costs.min(dim=2)  # Shape: (num_paths, num_positions)\n",
    "\n",
    "            # Update cost and path\n",
    "            grad_row = grad_map[y].unsqueeze(0).expand(num_paths, -1)  # Shape: (num_paths, W)\n",
    "            current_grad = -grad_row.gather(1, x_indices)\n",
    "            cost[y] = min_prev_costs + current_grad  # Shape: (num_paths, num_positions)\n",
    "            path[y] = neighbor_indices.gather(2, min_indices.unsqueeze(2)).squeeze(2)  # Shape: (num_paths, num_positions)\n",
    "\n",
    "        # Backtracking to find the optimal paths\n",
    "        idx = cost[-1].argmin(dim=1)  # Shape: (num_paths,)\n",
    "        optimal_paths = []\n",
    "        for y in reversed(range(H)):\n",
    "            optimal_paths.append(x_indices[torch.arange(num_paths), idx])  # Shape: (num_paths,)\n",
    "            idx = path[y, torch.arange(num_paths), idx]\n",
    "        optimal_paths = torch.stack(optimal_paths[::-1], dim=1)  # Shape: (num_paths, H)\n",
    "        return optimal_paths  # Shape: (num_paths, H)\n",
    "\n",
    "    def find_optimal_horizontal_paths(self, grad_map, y_inits, band_width):\n",
    "        \"\"\"\n",
    "        Find the optimal horizontal paths around the initial y positions using dynamic programming.\n",
    "        \"\"\"\n",
    "        H, W = grad_map.shape\n",
    "        device = grad_map.device\n",
    "        num_paths = y_inits.size(0)\n",
    "\n",
    "        # Define bands around y_inits\n",
    "        y_offsets = torch.arange(-band_width, band_width + 1, device=device)\n",
    "        y_indices = y_inits.unsqueeze(1) + y_offsets.unsqueeze(0)  # Shape: (num_paths, num_positions)\n",
    "        y_indices = y_indices.clamp(0, H - 1).long()\n",
    "        num_positions = y_indices.size(1)\n",
    "\n",
    "        # Initialize cost and path matrices\n",
    "        cost = torch.full((W, num_paths, num_positions), float('inf'), device=device)\n",
    "        path = torch.zeros((W, num_paths, num_positions), dtype=torch.long, device=device)\n",
    "\n",
    "        # First column\n",
    "        grad_col = grad_map[:, 0].unsqueeze(0).expand(num_paths, -1)  # Shape: (num_paths, H)\n",
    "        cost[0] = -grad_col.gather(1, y_indices)  # Shape: (num_paths, num_positions)\n",
    "\n",
    "        # Precompute position indices\n",
    "        positions = torch.arange(num_positions, device=device).unsqueeze(0).expand(num_paths, -1)  # Shape: (num_paths, num_positions)\n",
    "\n",
    "        # Dynamic programming\n",
    "        for x in range(1, W):\n",
    "            # Pad the previous cost for easy indexing\n",
    "            padded_prev_cost = torch.cat([\n",
    "                torch.full((num_paths, 1), float('inf'), device=device),\n",
    "                cost[x - 1],\n",
    "                torch.full((num_paths, 1), float('inf'), device=device)\n",
    "            ], dim=1)  # Shape: (num_paths, num_positions + 2)\n",
    "\n",
    "            # Indices for possible moves: up (-1), stay (0), down (+1)\n",
    "            move_offsets = torch.tensor([-1, 0, 1], device=device)\n",
    "            neighbor_indices = positions.unsqueeze(2) + move_offsets.view(1, 1, -1)  # Shape: (num_paths, num_positions, 3)\n",
    "            neighbor_indices = neighbor_indices.clamp(0, num_positions - 1)\n",
    "\n",
    "            # Adjust for padding\n",
    "            neighbor_indices_padded = neighbor_indices + 1  # Adjust for the padding\n",
    "            neighbor_indices_padded = neighbor_indices_padded.long()\n",
    "\n",
    "            # Adjust dimensions of padded_prev_cost\n",
    "            padded_prev_cost_expanded = padded_prev_cost.unsqueeze(1).expand(-1, num_positions, -1)\n",
    "\n",
    "            # Gather costs for possible moves\n",
    "            prev_costs = padded_prev_cost_expanded.gather(2, neighbor_indices_padded)  # Shape: (num_paths, num_positions, 3)\n",
    "\n",
    "            # Find the minimum cost among the neighbors\n",
    "            min_prev_costs, min_indices = prev_costs.min(dim=2)  # Shape: (num_paths, num_positions)\n",
    "\n",
    "            # Update cost and path\n",
    "            grad_col = grad_map[:, x].unsqueeze(0).expand(num_paths, -1)  # Shape: (num_paths, H)\n",
    "            current_grad = -grad_col.gather(1, y_indices)\n",
    "            cost[x] = min_prev_costs + current_grad  # Shape: (num_paths, num_positions)\n",
    "            path[x] = neighbor_indices.gather(2, min_indices.unsqueeze(2)).squeeze(2)  # Shape: (num_paths, num_positions)\n",
    "\n",
    "        # Backtracking to find the optimal paths\n",
    "        idx = cost[-1].argmin(dim=1)  # Shape: (num_paths,)\n",
    "        optimal_paths = []\n",
    "        for x in reversed(range(W)):\n",
    "            optimal_paths.append(y_indices[torch.arange(num_paths), idx])  # Shape: (num_paths,)\n",
    "            idx = path[x, torch.arange(num_paths), idx]\n",
    "        optimal_paths = torch.stack(optimal_paths[::-1], dim=1)  # Shape: (num_paths, W)\n",
    "        return optimal_paths  # Shape: (num_paths, W)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        if H != self.H or W != self.W:\n",
    "            raise ValueError(f\"Input image size must match initialized size: ({self.H}, {self.W})\")\n",
    "\n",
    "        # Compute gradient map\n",
    "        grad_map = self.compute_gradient_map(x)  # Shape: (B, 1, H, W)\n",
    "\n",
    "        # Initialize grid segmentation\n",
    "        segmentation_mask = self.initialize_grid(B)  # Shape: (B, H, W)\n",
    "\n",
    "        # Adjust boundaries\n",
    "        new_segmentation_mask = self.adjust_boundaries(grad_map, segmentation_mask)\n",
    "\n",
    "        return grad_map, segmentation_mask, new_segmentation_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "voronoi_model = VoronoiPropagation(num_clusters=256)\n",
    "pathFinder_model = BoundaryPathFinder2(num_segments_col=16,num_segments_row=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explained_variance_batch(image_batch, superpixel_labels_batch):\n",
    "    batch_size, num_channels, height, width = image_batch.shape\n",
    "    explained_variance_scores = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image = image_batch[i]  # Shape: (C, H, W)\n",
    "        superpixel_labels = superpixel_labels_batch[i]  # Shape: (H, W)\n",
    "\n",
    "        # Ensure superpixel_labels is in shape (H, W)\n",
    "        superpixel_labels = superpixel_labels.squeeze().to(image.device)\n",
    "\n",
    "        # Flatten image and labels for computation\n",
    "        image_flat = image.view(num_channels, height * width)\n",
    "        labels_flat = superpixel_labels.view(height * width)\n",
    "\n",
    "        # Compute total variance of the image across all channels\n",
    "        total_variance = image_flat.var(dim=1, unbiased=False).mean().item()\n",
    "\n",
    "        # Proceed to compute within-superpixel variance (homogeneity_score)\n",
    "        unique_labels = superpixel_labels.unique()\n",
    "        num_superpixels = unique_labels.size(0)\n",
    "\n",
    "        pixel_sums = torch.zeros((num_superpixels, num_channels), device=image.device)\n",
    "        pixel_squares = torch.zeros((num_superpixels, num_channels), device=image.device)\n",
    "        pixel_counts = torch.zeros(num_superpixels, device=image.device)\n",
    "\n",
    "        for j, label in enumerate(unique_labels):\n",
    "            mask = (labels_flat == label)\n",
    "            pixel_sums[j] = image_flat[:, mask].sum(dim=1)\n",
    "            pixel_squares[j] = (image_flat[:, mask] ** 2).sum(dim=1)\n",
    "            pixel_counts[j] = mask.sum()\n",
    "\n",
    "        pixel_means = pixel_sums / pixel_counts.unsqueeze(1)\n",
    "        pixel_variances = (pixel_squares / pixel_counts.unsqueeze(1)) - (pixel_means ** 2)\n",
    "        within_variance = pixel_variances.mean().item()\n",
    "\n",
    "        # Compute explained variance\n",
    "        explained_variance = 1 - (within_variance / total_variance)\n",
    "        explained_variance_scores.append(explained_variance)\n",
    "\n",
    "    return explained_variance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (image, labels) \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      7\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m     voronor_grad_map, voronor_centroids, voronor_mask, voronor_spixel_features \u001b[38;5;241m=\u001b[39m \u001b[43mvoronoi_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVoronoi took\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m-\u001b[39mstart)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mVoronoiPropagation.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    156\u001b[0m centroids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_nearest_minima(centroids, grad_map)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Use the color map (the original image) to guide propagation\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m#spixel_features = self.unet(x)\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Perform distance-weighted propagation with both gradient and color guidance\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance_weighted_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# return grad_map, centroids, mask, spixel_features\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad_map, centroids, mask, x\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mVoronoiPropagation.distance_weighted_propagation\u001b[1;34m(self, centroids, grad_map, color_map, num_iters, gradient_weight, color_weight, edge_exponent)\u001b[0m\n\u001b[0;32m    126\u001b[0m color_diffs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[0;32m    127\u001b[0m     torch\u001b[38;5;241m.\u001b[39mabs(color_map \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mroll(color_map, shifts\u001b[38;5;241m=\u001b[39ms, dims\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)))\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m shifts\n\u001b[0;32m    129\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Update distances\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m new_distances \u001b[38;5;241m=\u001b[39m \u001b[43mshifted_dist_maps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweighted_grad_map\u001b[49m \u001b[38;5;241m+\u001b[39m color_diffs \u001b[38;5;241m*\u001b[39m color_weight\n\u001b[0;32m    133\u001b[0m min_distances, min_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(new_distances, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Update mask and distance map where smaller distances are found\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "explained_variance_scores_voronoi = []\n",
    "explained_variance_scores_pathfinder = []\n",
    "\n",
    "for (image, labels) in dataloader:\n",
    "    start = time.time()\n",
    "    voronor_grad_map, voronor_centroids, voronor_mask, voronor_spixel_features = voronoi_model(image)\n",
    "    end = time.time()\n",
    "    print(\"Voronoi took\", end-start)\n",
    "    start = time.time()\n",
    "    pathfinder_grad_map, pathfinder_segmentation_mask, pathfinder_new_segmentation_mask = pathFinder_model(image)\n",
    "    end = time.time()\n",
    "    print(\"Pathfinder took\", end-start)\n",
    "    \n",
    "    explained_variance_scores_voronoi.append(explained_variance_batch(image, voronor_mask))\n",
    "    explained_variance_scores_pathfinder.append(explained_variance_batch(image, pathfinder_new_segmentation_mask))\n",
    "    \n",
    "\n",
    "print(np.mean(np.array(explained_variance_scores_voronoi)))\n",
    "print(np.mean(np.array(explained_variance_scores_pathfinder)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
